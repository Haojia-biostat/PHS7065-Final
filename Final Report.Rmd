---
title: "Prediction-Powered Inference Framework Under Covariate Shift"
subtitle: "PHS 7065 Fall 2024 - Final Report"
author: |
  | Instructor: Dr. Xuan Wang
  | Group Members: Yingjia Wei, Haojia Li
geometry: margin=2.5cm
output: 
  pdf_document:
    toc: true
    number_sections: true
date: "2024-12-14"
bibliography: references.bib
link-citations: true
---
 

# Methods

## PPI

Below are the the fundamental notations used in the PPI framework:

- Let $(X,Y) \in (\mathcal X \times \mathcal Y) ^n$ denote the labeled or the gold-standard dataset, where $X=(X_1,\ldots,X_n)$ and $Y=(Y_1,\ldots,Y_n)$.
- Similarly, let $(\tilde X, \tilde Y) \in (\mathcal X \times \mathcal Y)^N$ denote the unlabeled dataset, where the outcomes $\tilde Y$ are not observed.
- In this section, we assume that $(X,Y)$ and $(\tilde X, \tilde Y)$ are independently and identically distributed (i.i.d.) samples from a common distribution, $p(x,y)$.
- We have a prediction rule, $f:X \to Y$ that is independent of the observed data. Thus, $f(X_i)$ denote the predictions for the labeled data and $f(\tilde X_i)$ denote the predictions for the unlabeled data.

The goal of PPI is to construct a confidence interval $\mathcal C^{PP}$ for the estimand $\theta$.
We will focus on the mean outcome as the estimand in our study.
Specifically, we are interested in the mean outcome in the unlabeled data, $E[\tilde Y]$.
The key conceptual innovation of PPI lies in the introduction of measure of fit $m_\theta$ and rectifier $\Delta_\theta$:

- Measure of fit $m_\theta$ is computed on the predictions for the unlabeled data $(\tilde X, f(\tilde X))$ and quantifies how close the estimate of $\theta$ it to its true value.
- Rectifier $\Delta_\theta$ is defined as the difference of the measure of fit $m_\theta$ computed using the labeled data and its predictions $(X, Y, f(X))$. If the predictions are perfect, the rectifier is equal to zero.

The PPI method computes a confidence interval as $\mathcal C^{PP}_\alpha = \{ \theta \text{ such that} |m_\theta + \Delta_\theta| \leq t_\theta(\alpha) \}$, where $t_\theta(\alpha)$ is a constant depending on the error level $\alpha$.

When the estimand of interest is the mean outcome in unlabeled data, the algorithm for constructing the confidence interval $\mathcal C^{PP}_\alpha$ at error level $\alpha \in (0,1)$ is as follows:

1. Point estimate of $\theta$ by PPI: $\hat \theta^{PP} \leftarrow \tilde \theta^f - \hat \Delta := \frac{1}{N}\sum_{i=1}^{N} f(\tilde X_i) - \frac{1}{n}\sum_{i=1}^{n} (f(X_i)-Y_i)$
2. Empirical variance of estimate based on unlabeled data: $\hat \sigma_f^2 \leftarrow \frac{1}{N}\sum_{i=1}^{N} (f(\tilde X_i) - \tilde \theta^f)^2$
3. Empirical variance of the rectifier: $\hat \sigma_\Delta^2 \leftarrow \frac{1}{n}\sum_{i=1}^{n} (f(X_i) - Y_i - \hat \Delta)^2$
4. Normal approximation: $t_\alpha \leftarrow z_{1-\alpha/2} \sqrt{\frac{\hat \sigma_\Delta^2}{n} + \frac {\hat \sigma_f^2}{N}}$
5. Construct prediction-powered confidence set: $\mathcal C^{PP}_\alpha = \left( \hat \theta^{PP} \pm t_\alpha \right)$

## PPI++
The original PPI constructs a confidence set for $\theta^*$ by searching through all possible values of $\theta$. 
$$
\begin{aligned}
\hat \theta ^{PP}
& = L_n(\theta) + \tilde L^f_N(\theta) - L^f_n(\theta)\\
& = \frac{1}{N}\sum_{i=1}^{N} f(\tilde X_i) + \frac{1}{n}\sum_{i=1}^{n} Y_i - \frac{1}{n}\sum_{i=1}^{n} f(X_i)\\
& = \frac{1}{n}\sum_{i=1}^{n} Y_i + \left( \frac{1}{N}\sum_{i=1}^{N} f(\tilde X_i) - \frac{1}{n}\sum_{i=1}^{n} f(X_i) \right)
\end{aligned}
$$
Where $L(\theta)$ is the losses. It has a limitation: when the provided predictions are inaccurate, the intervals can be worse than the “classical intervals” using only the labeled data. To address this problem, we introduce a parameter $\lambda$:

$$
\begin{aligned}
\hat \theta ^{PP++}
& = L_n(\theta) + \lambda(\tilde L^f_N(\theta) - L^f_n(\theta)) \\
& = \frac{1}{n}\sum_{i=1}^{n} Y_i + \lambda \left( \frac{1}{N}\sum_{i=1}^{N} f(\tilde X_i) - \frac{1}{n}\sum_{i=1}^{n} f(X_i) \right)
\end{aligned}
$$

Tuning the parameter $\lambda$ to minimize the asymptotic variance of the prediction-powered estimator, we have:

$$
\begin{aligned}
\hat \lambda & = argmin_\lambda E\left\{
\frac{1}{n}\sum_{i=1}^{n} Y_i - \theta^* + 
\lambda \left[ \frac{1}{N}\sum_{i=1}^{N} f(\tilde X_i) - \frac{1}{n}\sum_{i=1}^{n} f(X_i) \right]\right\}^2 \\
& = \frac 
{Cov\left(\frac{1}{n}\sum_{i=1}^{n} Y_i, \frac{1}{N}\sum_{i=1}^{N} f(\tilde X_i) - \frac{1}{n}\sum_{i=1}^{n} f(X_i)\right)} 
{Var\left(\frac{1}{N}\sum_{i=1}^{N} f(\tilde X_i) - \frac{1}{n}\sum_{i=1}^{n} f(X_i)\right)} \\
& = \frac 
{\frac{1}{n} Cov\left(Y, f(X)\right)} 
{\frac{1}{N} Var\left(f(\tilde X)\right) + \frac{1}{n} Var\left(f(X)\right)}
\end{aligned}
$$


In this case, PPI++ is essentially never worse than either classical or original PPI. Certainly the objective remains unbiased for any $\lambda$, since $E[\tilde L^f_N(\theta) - L^f_n(\theta)]=0$, thus $E[L_n(\theta) + \tilde L^f_N(\theta) - L^f_n(\theta)]=L(\theta)$.





## Cross-PPI
Cross-prediction gives more stable inferences especially when the prediction model is not accurate. It begins by spliting the labeled data into $K$ chunks: $I_1=\left\{ 1,..., n/K \right\}, I_2= \left\{ n/K+1,...,2n/K \right\},...$ The cross-prediction estimator is defined as:

$$
\begin{aligned}
\hat \theta ^{Cross-PPI}
=&\frac{1}{n} \sum_{j=1}^{K}\sum_{i \in I_j}Y_i +
\frac{1}{KN}\sum_{j=1}^{K}\sum_{i=1}^{N} f^{(j)}(\tilde X_i) -
\frac{1}{n} \sum_{j=1}^{K}\sum_{i \in I_j}f^{(j)}(X_i) 
\end{aligned}
$$
where $f^{(j)}$ is the model obtained by training on all data except the $I_j$. Intuitively, the second term in $\hat \theta ^{Cross-PPI}$ is an empirical approximation of the population mean, the difference between first and third term is the rectifier which is an estimate of the bias between the predicted labels and the true labels. This cross-prediction estimator is also unbiased, as $E[f^{(j)}(\tilde X_{i'})]=E[f^{(j)}( X_{i})]$ for all $j\in [K], i \in I_j, i' \in [N]$.


## Cross-PPI++
Combining PPI++ and Cross-PPI, we can compute the cross-PPI++ estimator, defined as:


$$
\begin{aligned}
\hat \theta ^{Cross-PPI++}
=&\frac{1}{n} \sum_{j=1}^{K}\sum_{i \in I_j}Y_i +
\lambda \left[
\frac{1}{KN}\sum_{j=1}^{K}\sum_{i=1}^{N} f^{(j)}(\tilde X_i) -
\frac{1}{n} \sum_{j=1}^{K}\sum_{i \in I_j}f^{(j)}(X_i) \right]
\end{aligned}
$$
Similarly, the parameter $\lambda$ is tuned to minimize the asymptotic variance of the prediction-powered estimator. The optimal $\lambda$ is:

$$
\begin{aligned}
&\hat \lambda \text{ with k-fold cross-validation}\\ 
& = \frac {Cov\left(\frac{1}{n}\sum_{i=1}^{n} w(X_i) Y_i, \frac{1}{KN}\sum_{j=1}^{K}\sum_{i=1}^{N} f^{(j)}(\tilde X_i) -
\frac{1}{n} \sum_{j=1}^{K}\sum_{i \in I_j} w(X_i)f^{(j)}(X_i) \right)} {Var\left(\frac{1}{KN}\sum_{j=1}^{K}\sum_{i=1}^{N} f^{(j)}(\tilde X_i) -
\frac{1}{n} \sum_{j=1}^{K}\sum_{i \in I_j} w(X_i)f^{(j)}(X_i) \right)} \\
& = \frac{\frac{1}{n} Cov \left(w(X)Y, w(X)f^{(j)}(X) \right)}
{ \frac{1}{K^2N} \sum_{j=1}^{K} Var \left(f^{(j)}(\tilde X )\right) + \frac{1}{n^2} \sum_{j=1}^{K}\sum_{i \in I_j}  Var \left( w(X)f^{(j)}(X) \right) }
\end{aligned}
$$

# Extensions under Covariate Shift

## Standard PPI under Covariate shift

Although the foundational PPI framework assumes that the labeled and unlabeled data are drawn from the same distribution, this assumption does not always hold in practice.
One common violation is covariate shift, where the distribution of input features, $p(X)$, differs between the labeled and unlabeled dataset, while the conditional distribution of the outcome given the feature $p(Y|X)$ remains the same (Shimodaira, 2000).
To address this mismatch, the density ratio $w(x) = \tilde p(x)/ p(x)$ is introduced, where $\tilde p(x)$ is the distribution of the features in the unlabeled data and $p(x)$ is the distribution of the features in the gold-standard data (Sugiyama et al., 2007).
Incorporating the density ratio allows for adjusting model predictions to account for the covariate shift, ensuring valid and unbiased inference in scenarios where the covariate shift occurs.

Denote $w(x) = p^*(x)/\tilde p(x)$ the density ratio.

$$
\hat \theta \text{ by standard PPI under covariate shift}
= \frac{1}{n}\sum_{i=1}^{n} w(X_i) Y_i + \frac{1}{N}\sum_{i=1}^{N} f(\tilde X_i) - \frac{1}{n}\sum_{i=1}^{n} w(X_i)f(X_i) \\
$$

## Efficient PPI under Covariate shift

Efficient PPI estimator adjusted by density ratio $w(x) = \tilde p(x)/ p(x)$ is:

$$
\hat \theta \text{ by Efficient PPI under covariate shift} = \frac{1}{n}\sum_{i=1}^{n} w(X_i) Y_i + 
\lambda \left[ \frac{1}{N}\sum_{i=1}^{N} f(\tilde X_i) - \frac{1}{n}\sum_{i=1}^{n} w(X_i)f(X_i) \right]
$$
We can find the parameter $\lambda$ at:

$$
\begin{aligned}
\hat \lambda & = argmin_\lambda E\left\{
\frac{1}{n}\sum_{i=1}^{n} w(X_i) Y_i - \theta^* + 
\lambda \left[ \frac{1}{N}\sum_{i=1}^{N} f(\tilde X_i) - \frac{1}{n}\sum_{i=1}^{n} w(X_i)f(X_i) \right]\right\}^2 \\
& = \frac 
{Cov\left(\frac{1}{n}\sum_{i=1}^{n} w(X_i) Y_i, \frac{1}{N}\sum_{i=1}^{N} f(\tilde X_i) - \frac{1}{n}\sum_{i=1}^{n} w(X_i)f(X_i)\right)} 
{Var\left(\frac{1}{N}\sum_{i=1}^{N} f(\tilde X_i) - \frac{1}{n}\sum_{i=1}^{n} w(X_i)f(X_i)\right)} \\
& = \frac 
{\frac{1}{n} Cov\left(w(X)Y, w(X)f(X)\right)} 
{\frac{1}{N} Var\left(f(\tilde X)\right) + \frac{1}{n} Var\left(w(X)f(X)\right)}
\end{aligned}
$$



## Cross Efficient PPI under Covariate Shift

We can compute the cross-prediction estimator using the trained models, consider the covariate shift situation and adding density ratio, the estimator by Cross-PPI is defined as:

$$
\begin{aligned}
& \hat \theta \text{ by Cross PPI under covariate shift}\\
=&\frac{1}{n} \sum_{j=1}^{K}\sum_{i \in I_j} w(X_i)Y_i +
\frac{1}{KN}\sum_{j=1}^{K}\sum_{i=1}^{N} f^{(j)}(\tilde X_i) -
\frac{1}{n} \sum_{j=1}^{K}\sum_{i \in I_j} w(X_i)f^{(j)}(X_i)
\end{aligned}
$$

Adding the parameter $\lambda$ to minimize the asymptotic variance of the prediction-powered estimator under this covariate shift situation, we have:

$$
\begin{aligned}
& \hat \theta \text{ by Cross Efficient PPI under covariate shift}\\
=&\frac{1}{n} \sum_{j=1}^{K}\sum_{i \in I_j} w(X_i)Y_i +
\lambda \left[
\frac{1}{KN}\sum_{j=1}^{K}\sum_{i=1}^{N} f^{(j)}(\tilde X_i) -
\frac{1}{n} \sum_{j=1}^{K}\sum_{i \in I_j} w(X_i)f^{(j)}(X_i) \right]
\end{aligned}
$$
Thus, $\lambda$ can be found at:

$$
\begin{aligned}
& \hat \lambda \text{ with k-fold cross-validation} \\
=& \frac {Cov\left(\frac{1}{n}\sum_{i=1}^{n} w(X_i) Y_i, \frac{1}{KN}\sum_{j=1}^{K}\sum_{i=1}^{N} f^{(j)}(\tilde X_i) -
\frac{1}{n} \sum_{j=1}^{K}\sum_{i \in I_j} w(X_i)f^{(j)}(X_i) \right)} {Var\left(\frac{1}{KN}\sum_{j=1}^{K}\sum_{i=1}^{N} f^{(j)}(\tilde X_i) -
\frac{1}{n} \sum_{j=1}^{K}\sum_{i \in I_j} w(X_i)f^{(j)}(X_i) \right)} \\
=& \frac{\frac{1}{n} Cov \left(w(X)Y, w(X)f^{(j)}(X) \right)}
{ \frac{1}{K^2N} \sum_{j=1}^{K} Var \left(f^{(j)}(\tilde X )\right) + \frac{1}{n} Var \left(w(X)f^{(\cdot)}(X)\right)}
\end{aligned}
$$


# Results

# Conclusion
We compared the performance of different PPI frameworks under different scenarios of covariate shifts. Efficient PPI and Efficient cross-PPI have relatively narrower CI widths compared to standard PPI and CLT. When the covariate shift is mild and moderate, most of the PPI frameworks can maintain valid, with coverage over or close to 95%. However, when the difference between two distributions are large, only standard PPI has coverage validity, especially when labeled sample size is small. Another situation that PPI will be under powered is when the prediction model is not accurate, which is simulated as higher noise level in our analysis. The coverage of all PPI frameworks decrease when noise level increases, but standard PPI is again the best across all frameworks. Thus, Efficient PPI frameworks provide more precise parameter estimates while maintaining robustness under mild to moderate covariate shifts and noise level. 


# Discussion
Density ratio weighting performs well when the labeled and unlabeled data have relatively similar distributions. In such cases, PPI frameworks with density ratio weighting can achieve narrow and valid confidence interval estimation. However, estimating density ratio in real-world data could be challenging, where the data-generating process is complex and the true underlying distributions are unknown. Future work could explore the use of more robust techniques for density ratio estimation, such as kernel-based methods or machine learning approaches, to improve reliability in diverse real-world scenarios.

Our study also highlights the trade-off between efficiency and validity across different PPI frameworks. Both adding parameter $\lambda$ and involving cross-prediction can improve efficiency, however, they may loss validity when covariate shift is severe and noise level in data is high. Standard PPI always maintain the highest overall coverage rate in different scenarios, and this robustness is particularly important in real-world applications. Therefore, the choice of PPI framework depends on the nature of application and data, balancing the efficiency and validity for specific scientific questions. 
